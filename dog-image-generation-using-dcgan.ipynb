{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":15062,"databundleVersionId":545987,"sourceType":"competition"}],"dockerImageVersionId":30121,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\ngen_dog_imgs = '/kaggle/working/generative-dog-images'\nif not os.path.exists(gen_dog_imgs):\n    os.makedirs(gen_dog_imgs)\n\ndogs_dir = '/kaggle/working/dogs'\nif not os.path.exists(dogs_dir):\n    os.makedirs(dogs_dir)    \n\n# !mkdir /kaggle/working/generative-dog-images\n!unzip /kaggle/input/generative-dog-images/all-dogs.zip -d /kaggle/working/generative-dog-images > /dev/null 2>&1\n!unzip /kaggle/input/generative-dog-images/Annotation.zip -d /kaggle/working/generative-dog-images > /dev/null 2>&1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generative Adversarial Network","metadata":{}},{"cell_type":"markdown","source":"# Generative Adversarial Network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative AI. In a GAN, two neural networks, generator and discriminator, contest with each other in the form of a zero-sum game, where one agent's gain is another agent's loss. The core idea of a GAN is to train the generator to \"fool\" the discriminator rather than directly minimize the individual image distances, and the discriminator is indirectly trained to tell how realistic the generated images may seem. This way, both the generator and the discriminator are updated dynamically against each other to achieve realistic imitation to the original images.\n\n# GAN aims to learn to generate new data with the same statistics as the provided training set. A GAN trained on photographs can generate new iamges superficially authentic to human observers. While GAN is originally intended to be implemented in unsupervised learning, variations of GANs are developed into models suitable for semi-supervised and supervised learning purposes.","metadata":{}},{"cell_type":"markdown","source":"# Dataset Overview","metadata":{}},{"cell_type":"markdown","source":"# The Stanford Dogs dataset contains images of 120 breeds of dogs worldwide. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization. There are 20,580 images, out of which 12,000 are used for training and 8580 for testing. Class labels and bounding box annotations are provided for all the 12,000 images. In this study, training and testing images are not distinguished as they are repurposed into image generation.","metadata":{}},{"cell_type":"code","source":"#%matplotlib inline\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport random\nfrom PIL import Image\nimport xml.etree.ElementTree as ET ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set random seed for reproducibility\nmanualSeed = 999\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)\ntorch.set_deterministic(True) # Needed for reproducible results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_dogs_dir = '/kaggle/working/generative-dog-images/all-dogs'\nannotation_dir = '/kaggle/working/generative-dog-images/Annotation'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_bndbox(filename, square=False):\n    tree = ET.parse(filename)\n    root = tree.getroot()\n    box = root.find('object').find('bndbox')\n    xmin = int(box.find('xmin').text)\n    ymin = int(box.find('ymin').text)\n    xmax = int(box.find('xmax').text)\n    ymax = int(box.find('ymax').text)\n    \n    if square:\n        center_x, center_y = (xmin + xmax)//2, (ymin+ymax)//2\n        max_w = max(xmax-xmin, ymax-ymin)\n        xmin = center_x - max_w//2\n        xmax = xmin + max_w\n        ymin = center_y - max_w//2\n        ymax = ymin + max_w\n        \n    return xmin, ymin, xmax, ymax  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from https://www.kaggle.com/korovai/dogs-images-intruders-extraction\nintruders = [\n    #n02088238-basset\n    'n02088238_10870_0.jpg',\n    \n    #n02088466-bloodhound\n    'n02088466_6901_1.jpg',\n    'n02088466_6963_0.jpg',\n    'n02088466_9167_0.jpg',\n    'n02088466_9167_1.jpg',\n    'n02088466_9167_2.jpg',\n    \n    #n02089867-Walker_hound\n    'n02089867_2221_0.jpg',\n    'n02089867_2227_1.jpg',\n    \n    #n02089973-English_foxhound # No details\n    'n02089973_1132_3.jpg',\n    'n02089973_1352_3.jpg',\n    'n02089973_1458_1.jpg',\n    'n02089973_1799_2.jpg',\n    'n02089973_2791_3.jpg',\n    'n02089973_4055_0.jpg',\n    'n02089973_4185_1.jpg',\n    'n02089973_4185_2.jpg',\n    \n    #n02090379-redbone\n    'n02090379_4673_1.jpg',\n    'n02090379_4875_1.jpg',\n    \n    #n02090622-borzoi # Confusing\n    'n02090622_7705_1.jpg',\n    'n02090622_9358_1.jpg',\n    'n02090622_9883_1.jpg',\n    \n    #n02090721-Irish_wolfhound # very small\n    'n02090721_209_1.jpg',\n    'n02090721_1222_1.jpg',\n    'n02090721_1534_1.jpg',\n    'n02090721_1835_1.jpg',\n    'n02090721_3999_1.jpg',\n    'n02090721_4089_1.jpg',\n    'n02090721_4276_2.jpg',\n    \n    #n02091032-Italian_greyhound\n    'n02091032_722_1.jpg',\n    'n02091032_745_1.jpg',\n    'n02091032_1773_0.jpg',\n    'n02091032_9592_0.jpg',\n    \n    #n02091134-whippet\n    'n02091134_2349_1.jpg',\n    'n02091134_14246_2.jpg',\n    \n    #n02091244-Ibizan_hound\n    'n02091244_583_1.jpg',\n    'n02091244_2407_0.jpg',\n    'n02091244_3438_1.jpg',\n    'n02091244_5639_1.jpg',\n    'n02091244_5639_2.jpg',\n    \n    #n02091467-Norwegian_elkhound\n    'n02091467_473_0.jpg',\n    'n02091467_4386_1.jpg',\n    'n02091467_4427_1.jpg',\n    'n02091467_4558_1.jpg',\n    'n02091467_4560_1.jpg',\n    \n    #n02091635-otterhound\n    'n02091635_1192_1.jpg',\n    'n02091635_4422_0.jpg',\n    \n    #n02091831-Saluki\n    'n02091831_1594_1.jpg',\n    'n02091831_2880_0.jpg',\n    'n02091831_7237_1.jpg',\n    \n    #n02092002-Scottish_deerhound\n    'n02092002_1551_1.jpg',\n    'n02092002_1937_1.jpg',\n    'n02092002_4218_0.jpg',\n    'n02092002_4596_0.jpg',\n    'n02092002_5246_1.jpg',\n    'n02092002_6518_0.jpg',\n    \n    #02093256-Staffordshire_bullterrier\n    'n02093256_1826_1.jpg',\n    'n02093256_4997_0.jpg',\n    'n02093256_14914_0.jpg',\n    \n    #n02093428-American_Staffordshire_terrier\n    'n02093428_5662_0.jpg',\n    'n02093428_6949_1.jpg'\n            ]\n\nlen(intruders)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndogs_count = 0\nfor breed in os.listdir(annotation_dir):\n    for dog in os.listdir(os.path.join(annotation_dir, breed)):\n        #print(dog)\n        bndbox = get_bndbox(os.path.join(annotation_dir, breed, dog), square=True)\n        jpg_name = os.path.join(all_dogs_dir, dog+'.jpg')\n        intruders_join = '\\t'.join(intruders)\n        if os.path.exists(jpg_name):     \n            if dog not in intruders_join:\n                img = Image.open(jpg_name).crop(bndbox)\n                img.save(os.path.join(dogs_dir, dog+'.jpg'))\n                dogs_count+=1\nprint('number of dogs in the original dataset:', len(os.listdir(dogs_dir))+len(intruders))\nprint('number of dogs in the dataset excluding intruders:',dogs_count)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass DogDataset(Dataset):\n    def __init__(self, data_dir, transforms=None):\n        self.files = [os.path.join(data_dir, file) for file in os.listdir(data_dir)]\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, index):\n        img = Image.open(self.files[index])\n        if self.transforms is not None:\n            img = self.transforms(img)\n        return img","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_size = (64, 64)\nimage_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(img_size),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nbatch_size = 128\ntrainloader = DataLoader(\n    DogDataset(data_dir=dogs_dir, transforms=image_transforms),\n    batch_size = batch_size,\n    shuffle = True,\n    num_workers = 3,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DCGAN","metadata":{}},{"cell_type":"markdown","source":"# DCGAN (Deep Convolutional GAN) is a generative adversarial network architecture using deep convolutional neural networks. It is specialized for generating realistic images, mainly square images, in computer vision field. DCGAN can learn and capture detailed features in images of the original training dataset to generate realistic fake images hardly distinguishable by human eyes.","metadata":{}},{"cell_type":"markdown","source":"# A great DCGAN tutorial can be founded in: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, z_channels, out_channels=3):\n        super(Generator, self).__init__()\n        \n        convs = []\n        channels = [z_channels, 1024, 512, 256, 128, 64]\n        for i in range(1, len(channels)):\n            convs.append(nn.ConvTranspose2d(channels[i-1], channels[i], 2, stride=2, bias=False))\n            convs.append(nn.BatchNorm2d(channels[i]))\n            #convs.append(nn.ReLU(inplace=True))\n            convs.append(nn.LeakyReLU(0.1, inplace=True))\n        convs.append(nn.ConvTranspose2d(channels[-1], out_channels, 2, stride=2, bias=False))\n        convs.append(nn.Tanh())\n        \n        self.convs = nn.Sequential(*convs)\n        \n    def forward(self, x):\n        return self.convs(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        channels = [3, 64, 128, 256, 512]\n        convs = []\n        for i in range(1, len(channels)):\n            convs.append(nn.Conv2d(channels[i-1], channels[i], 3, padding=1, stride=2, bias=False))\n            if i != 1:\n                convs.append(nn.BatchNorm2d(channels[i]))\n            convs.append(nn.LeakyReLU(0.2, inplace=True))\n        \n        convs.append(nn.Conv2d(channels[-1], 1, 4, bias=False))\n        convs.append(nn.Sigmoid())\n        \n        self.convs = nn.Sequential(*convs)\n    \n    def forward(self, x):\n        x = self.convs(x)\n        return x.view(-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def weights_init(m):\n#    classname = m.__class__.__name__\n#    if classname.find('Conv') != -1:\n#        m.weight.data.normal_(0.0, 0.02)\n#    elif classname.find('BatchNorm') != -1:\n#        m.weight.data.normal_(1.0, 0.02)\n#        m.bias.data.fill_(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# custom weights initialization called on ``netG`` and ``netD``\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"z_channels = 100\nG = Generator(z_channels, 3)\nG.apply(weights_init)\nD = Discriminator()\nD.apply(weights_init)\ncriterion = nn.BCELoss()\n\ncuda = torch.cuda.is_available()\nif cuda:\n    print('Use GPU')\n    G = G.cuda()\n    D = D.cuda()\n    criterion = criterion.cuda()\nelse:\n    print('No GPU')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lr = 0.0002\n#b1,b2 = 0.5,0.999\n\nlr = 0.0004\nb1,b2 = 0.6,0.999\n\noptimizerG = torch.optim.AdamW(G.parameters(), lr=lr, betas=(b1, b2))\noptimizerD = torch.optim.AdamW(D.parameters(), lr=lr, betas=(b1, b2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fixed_noise = torch.normal(0, 0.1, size=(64, z_channels, 1, 1))\nif cuda:\n    fixed_noise = fixed_noise.cuda()\n\n#epoches = 100\nepoches = 20\ngenerate_imgs = []\nG_losses, D_losses = [],[]\nfor epoch in range(epoches):\n    for i, img in enumerate(trainloader):\n        z = torch.normal(0, 0.1, size=(img.size(0), z_channels, 1, 1))\n        real = torch.ones(img.size(0))\n        fake = torch.zeros(img.size(0))\n        if cuda:\n            img, z = img.cuda(), z.cuda()\n            real, fake = real.cuda(), fake.cuda()\n\n        # train D\n        D.zero_grad()\n        loss_real = criterion(D(img), real)\n        loss_real.backward()\n\n        fake_img = G(z)\n        loss_fake = criterion(D(fake_img.detach()), fake)\n        loss_fake.backward()\n\n        loss_D = (loss_real + loss_fake) / 2\n\n        optimizerD.step()\n\n        # train G\n        G.zero_grad()\n        loss_G = criterion(D(fake_img), real)\n        loss_G.backward()\n        optimizerG.step()\n\n    with torch.no_grad():\n        noise_img = G(fixed_noise)\n        generate_imgs.append(noise_img)\n        print(f'[Epoch {epoch+1}/{epoches}] [G loss: {loss_G.item()}] [D loss: {loss_D.item()} | loss_real: {loss_real.item()} loss_fake: {loss_fake.item()}]')\n        G_losses.append(loss_G.item()) \n        D_losses.append(loss_D.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\nx_epoches = list(range(1,epoches+1))\nplt.plot(x_epoches, G_losses, 'b-o', x_epoches, D_losses, 'r-x')\nplt.legend(['Generator Loss','Discriminator Loss'])\nplt.title('Optimizer Loss vs Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.animation as animation\nfrom IPython.display import HTML\n\nfig = plt.figure(figsize=(15,15))\nplt.axis(\"off\")\n\nimgs = []\nfor batch_images in generate_imgs:\n    imgs.append([plt.imshow(make_grid(batch_images[:64], padding=2, normalize=True).cpu().permute(1,2,0))])\n\nani = animation.ArtistAnimation(fig, imgs, interval=1000, repeat_delay=1000, blit=True)\nHTML(ani.to_jshtml())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}